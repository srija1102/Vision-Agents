# Interview Pressure Analyzer

A real-time multimodal AI agent that sits inside a live video interview and simultaneously analyzes both participants â€” measuring candidate stress, interviewer aggression, and conversation dynamics â€” then streams the results to a live browser dashboard with coaching feedback, spike alerts, session history, and a full post-session performance summary.

Built on [Vision Agents](https://github.com/GetStream/Vision-Agents) for the Vision Agents Hackathon.

---

## What It Does

The agent joins a video call via Stream WebRTC and processes three input streams at once:

- **Candidate video** â€” YOLOv11 pose estimation at 15 fps tracks body posture, shoulder tension, and movement patterns
- **Candidate audio** â€” Deepgram STT with eager turn detection transcribes speech in real-time and measures filler words, speech rate, pitch variance, and pause patterns
- **Conversation turns** â€” InterviewBridge tracks who is speaking, interruption patterns, and question cadence

Every 5 seconds, all signals are fused into a behavioral analysis window and pushed to the browser dashboard over WebSocket in under 2 seconds end-to-end.

### What Gets Measured

**Candidate side (11 stress signals):**
Speech rate deviation, filler word density (um/uh/like), pitch variance, pause frequency, answer latency, syllable stress patterns, topic complexity response, turn interruptions received, posture confidence, facial tension, response length deviation.

**Interviewer side (5 aggression signals):**
Interruption rate, rapid-fire question density, challenging question ratio, speaking dominance ratio, tone escalation.

**Together:**
Conversation imbalance score, dominant party, pressure trend (rising / stable / falling), risk flag (none / moderate / high), and live coaching feedback generated by Claude Haiku.

### Scoring Architecture

Scores are computed in two stages:

1. **Deterministic scorer** â€” weighted sum of all signals (weights sum to 1.0 per dimension). A multi-signal gate caps stress at 50% and aggression at 60% if fewer than 2 signals are active, preventing single-feature false positives.
2. **LLM refiner** â€” Claude Haiku receives the pre-computed scores as anchors and can adjust them within Â±0.15 bounds, then generates the coaching tip. The LLM never produces scores from scratch â€” it can only refine.

After 60 seconds of audio/video, a z-score baseline kicks in so each candidate is normalized against their own personal baseline, not a global average.

---

## Dashboard

The dashboard runs at `http://localhost:8080` and stays open after the session ends for review.

| Panel | What it shows |
|-------|---------------|
| **Candidate Stress** | 0â€“100% stress score with confidence level |
| **Interviewer Aggression** | 0â€“100% aggression score with confidence level |
| **Risk Level** | None / Moderate / High with animated border on high risk |
| **Resilience** | How well the candidate recovers after stress spikes |
| **Chart** | Rolling 30-window time series of stress and aggression |
| **Coaching Feedback** | Live actionable tip updated every 5 seconds |
| **Spike Log** | Detected stress spikes with Ïƒ magnitude and cause |
| **Session Metrics** | Interviewer style badge, dominant party, imbalance score |
| **Conversation Transcript** | Live Q&A feed â€” every candidate utterance and AI response |
| **Session Summary** | Full overlay on session end: peak stress, average stress, resilience, spike count, filler rate, strengths, areas to improve |
| **ğŸ“ History** | Button opens a modal listing every past session â€” click any to replay its full data into the dashboard |

### Persistence

Every session is stored in a SQLite database at `sessions/interviews.db`. The database captures:
- Every 5-second analysis window (stress, aggression, risk, coaching, all signals)
- Every spike event with cause and Ïƒ magnitude
- Full conversation transcript with speaker labels and timestamps
- Session summary with strengths and improvement areas
- Session comparison deltas vs the previous run

Reloading the dashboard page during a live session restores all current data instantly (buffered server-side). After the session ends, any past session can be loaded from the History panel at any time.

---

## Smart Features

**Auto-summary on leave** â€” the moment the candidate closes the interview tab, the full session summary is pushed to the dashboard immediately. No need to wait for the call to formally end.

**Session comparison** â€” on the second run with the same call-id, the summary overlay shows per-metric delta percentages vs the previous session (e.g. "Stress â†“ 14%", "Resilience â†‘ 22%").

**Graceful degradation** â€” if STT, video, or the LLM is unavailable, the system falls back to the deterministic result rather than failing.

**Skip-not-queue** â€” if the previous analysis cycle is still running when the next one is due, it is skipped rather than queued. This prevents a backlog of stale analysis windows.

---

## Setup

### Requirements

- Python 3.12
- `uv` package manager
- A [GetStream](https://getstream.io) account (free tier works)
- API keys: OpenRouter, Deepgram, ElevenLabs

### Install

```bash
# From the repo root
uv venv --python 3.12
uv sync --all-extras --dev
cp env.example .env
```

### Environment Variables

Fill in `.env` with your credentials:

```env
STREAM_API_KEY=...
STREAM_API_SECRET=...
OPENROUTER_API_KEY=...
DEEPGRAM_API_KEY=...
ELEVENLABS_API_KEY=...
```

Optional â€” override which models are used:

```env
INTERVIEWER_MODEL=anthropic/claude-haiku-4-5   # voice of the AI interviewer
ANALYSIS_MODEL=anthropic/claude-haiku-4-5      # behavioral analysis LLM
```

Both default to `anthropic/claude-haiku-4-5` via OpenRouter. The analysis model uses ~400 tokens per 5-second window. The interviewer model handles conversation turns.

---

## Running

### Option 1 â€” Dashboard Preview (no credentials needed)

Replays a synthetic 90-second adversarial interview through the real dashboard so you can see all panels working without a live call or API keys:

```bash
uv run python examples/09_interview_pressure_analyzer/preview_dashboard.py
```

Open `http://localhost:8080`. The replay runs 18 windows at 1.2-second intervals, simulating rising stress, aggression spikes, and a full session summary at the end.

### Option 2 â€” Live Interview Agent

The agent joins a real Stream video call, the AI conducts the interview, and analysis runs in real-time:

```bash
uv run python examples/09_interview_pressure_analyzer/interview_analyzer.py \
    --call-id <your-call-id>
```

Two browser tabs open automatically:
- `http://localhost:8080` â€” the live dashboard
- The Stream join link â€” where the candidate connects

The candidate can join from any device using Stream's web, iOS, Android, Flutter, or React Native SDK.

**Optional flags:**

```
--candidate-user-id   Stream user ID of the candidate (auto-detected if omitted)
--port                Dashboard port (default: 8080)
```

When the candidate leaves the call, the session summary appears on the dashboard immediately. The server stays running (Ctrl-C to quit) so you can review performance, browse the transcript, and compare against past sessions.

---

## Architecture

```
Candidate video  â†’ YOLOv11 pose (15 fps) â†’ PoseBridge  â”€â”€â”
Candidate audio  â†’ Deepgram STT          â†’ InterviewBridgeâ”€â”¤â†’ BehavioralAnalyzer
Turn events      â†’ InterviewBridge       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Every 5 seconds:
  BehavioralAnalyzer
    â†’ deterministic weighted scorer (11 stress + 5 aggression signals)
    â†’ 60s per-session z-score baseline normalizer
    â†’ multi-signal gate (prevents false positives)
    â†’ Claude Haiku anchored refiner (adjusts Â±0.15, generates coaching tip)
    â†’ BehavioralAnalysisEvent emitted on agent event bus

  BehavioralAnalysisEvent
    â†’ WebSocket â†’ Browser dashboard
    â†’ SQLite (sessions/interviews.db)
    â†’ Terminal log

On participant leave / call end:
  SpikeDetector + ResilienceTracker + SessionSummaryGenerator
    â†’ SessionSummaryEvent
    â†’ WebSocket â†’ Summary overlay
    â†’ SQLite (summaries table)
    â†’ sessions/<id>_<timestamp>.json (backup)
```

### Module Map

| Module | Role |
|--------|------|
| `interview_analyzer.py` | Main entry point â€” wires the agent, dashboard, and all processors |
| `preview_dashboard.py` | Synthetic replay â€” demos the dashboard without a live call |
| `plugins/behavioral_analyzer/analyzer.py` | Orchestrates the full analysis pipeline |
| `_scorer.py` | Deterministic weighted signal fusion |
| `_baseline.py` | Per-session z-score normalization with 60s calibration window |
| `_aggregator.py` | Buffers 5-second windows of audio/video metrics |
| `_spike_detector.py` | Detects >2Ïƒ stress spikes, tags cause (question / interruption / silence) |
| `_resilience.py` | Tracks post-spike stress recovery, computes resilience score |
| `_aggression_classifier.py` | Classifies interviewer style: neutral / challenging / rapid_fire / interruptive / hostile |
| `_session_summary.py` | Generates end-of-session summary with strengths and improvement areas |
| `_session_comparison.py` | Per-metric delta percentages between two sessions |
| `_explainability.py` | Signal contribution breakdown and audit log |
| `_degradation.py` | Graceful fallback policy when components are unavailable |
| `interview_bridge.py` | Routes STT transcripts and turn events into the analyzer |
| `pose_bridge.py` | Routes YOLO pose keypoints into the analyzer |

---

## Hackathon Alignment

| Requirement | How this satisfies it |
|-------------|-----------------------|
| Multi-modal agents that watch, listen, and understand video | YOLOv11 pose (video) + Deepgram STT (audio) + Claude Haiku reasoning, all in real-time |
| <30ms audio/video latency | Stream WebRTC edge network |
| <500ms join latency | Stream edge |
| Combine YOLO + LLM in real-time | YOLOv11 pose â†’ BehavioralAnalyzer â†’ Claude Haiku, every 5 seconds |
| Native LLM APIs | Claude Haiku via OpenRouter (native Anthropic Chat Completions API) |
| Cross-platform candidate SDK | Candidate joins via Stream's web, iOS, Android, Flutter, or React Native SDK |

---

## Tests

```bash
uv run py.test plugins/behavioral_analyzer/tests/ -q
# Expected: 74 passed
```
